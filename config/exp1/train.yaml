name: VideoMAE-3DTube-HNeRV-3DDecoder

dataloader:
  type: VideoDataLoader

  args:
    batch_size: 1 
    shuffle: True
    num_workers: 4 
    pin_memory: True
    drop_last: False
    device: 'cuda'

  data_path: "all/data/beauty30.mp4"
  vid: k400_train0       # video id
  data_split: 1_1_1      # Valid_train/total_train/all data split, e.g., 18_19_20 means for every 20 samples, the first 19 samples is full train set, and the first 18 samples is chose currently
  crop_list: "960_1920"    # video crop size
  resize_list: -1        # video resize size

  shuffle_data: False    # randomly shuffle the frame idx


model: 
  # VideoMAE 
  type: VisionTransformer

  args: 
    model: vit_base_patch16_224 
    pretrained: False
    all_frames: args.num_frames * args.num_segments
    tubelet_size: args.tubelet_size,
    fc_drop_rate: args.fc_drop_rate,
    drop_rate: args.drop,
    drop_path_rate: args.drop_path,
    attn_drop_rate: args.attn_drop_rate,
    drop_block_rate: None,
    use_checkpoint: args.use_checkpoint,
    use_mean_pooling: args.use_mean_pooling,
    init_scale: args.init_scale, 

  # HNeRV embedding and encoding parameters
  type: HNeRVEncoder

  args:
    embed: # empty string for HNeRV, and base value/embed_length for NeRV position encoding
    enc_dim: 64_16 # enc latent dim and embedding ratio
    
    ks: 0_3_3 # kernel size for encoder and decoder
    enc_strds: [] # stride list for encoder
    model_size: 1.5 # model parameters size: model size + embedding parameters
    saturate_stages: -1 # saturate stages for model size computation
  
  # HNeRV decoding parameters: FC + Conv
  type: HNeRVDecoder

  args:
    fc_hw: 9_16 # out size (h,w) for mlp
    reduce: 1.2 # chanel reduction for next stage
    lower_width: 32 # lowest channel width for output feature maps
    dec_strides: [5, 3, 2, 2, 2] # strides list for decoder
    num_blocks: 1_1 # block number for encoder and decoder
    conv_type: ["convnext", "pshuffel"] # conv type for encoder/decoder, choices=["pshuffel", "conv", "convnext", "interpolate"]
    norm: None # norm layer for generator", choices=["none", "bn", "in"]
    activation: gelu # activation to use",choices=["relu","leaky","leaky01","relu6","gelu","swish","softplus","hardswish"]


optimizer:
  type: AdamW

  args: 
    learning_rate: 6e-4 # max learning rate
    max_iters: 600000 # total number of training iterations
    weight_decay: 1e-1
    beta1: 0.9
    beta2: 0.95
    grad_clip: 1.0 # clip gradients at this value, or disable if == 0.0
  
  type: LRDecay

  args:
    decay_lr: True # whether to decay the learning rate
    warmup_iters: 2000 # how many steps to warm up for
    lr_decay_iters: 600000 # should be ~= max_iters per Chinchilla
    min_lr: 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

    lr: 0.001
  lr_type: cosine_0.1_1_0.1 # learning rate type, default=cosine


trainer:
  type: Trainer

  not_resume: False # not resume from latest checkpoint

  block_params: 1_1 # residual blocks and percentile to save

  out_bias: tanh # using sigmoid/tanh/0.5 for output prediction

  loss: Fusion6

  compile: True # use PyTorch 2.0 to compile the model to be faster
  
  start_epoch: -1 
  epochs: 1000
  save_dir: saved/
  save_period: 10
  verbosity: 1

  visual_tool: wandb
  project: aaai24

  api_key_file: ./configs/api/tuanlda78202
  entity: tuanlda78202


eval: 
  eval_only: False
  eval_freq: 10 # evaluation frequency
  eval_fps: False # fwd multiple times to test the fps

  quant_model_bit: 8 # bit length for model quantization
  quant_embed_bit: 6 # bit length for embedding quantization
  quant_axis: 0     # quantization axis (-1 means per tensor)

  dump_images: False # dump the prediction images
  dump_videos: False # concat the prediction images into video

  encoder_file: ""  # specify the embedding file


log: 
  # logging, output directory
  debug: False # defbug status, earlier for train/eval
  print-freq: 50 

  weight: None # pre-trained weights for ininitialization
  overwrite: False # overwrite the output dir if already exists

  outf: unify # folder to output images and model checkpoints
  suffix: "" # suffix str for outf


